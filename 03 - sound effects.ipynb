{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bce6ce95-d6a7-4ab5-80e8-576cbd2592f2",
      "metadata": {
        "id": "bce6ce95-d6a7-4ab5-80e8-576cbd2592f2"
      },
      "source": [
        "# Sound Effects\n",
        "\n",
        "Let's implement some simple sound effects in Numpy and then translate them to use the array API."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparations\n",
        "\n",
        "We will need a simple guitar sound file and install a helper library for later."
      ],
      "metadata": {
        "id": "Uv8lE62gR8vS"
      },
      "id": "Uv8lE62gR8vS"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/betatim/sound-array-api-tutorial/raw/main/guitar.wav\n",
        "!pip install array-api-compat ipywebrtc"
      ],
      "metadata": {
        "id": "e1eNAsadSHhC"
      },
      "id": "e1eNAsadSHhC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing a Tremolo Effect"
      ],
      "metadata": {
        "id": "o0keMH__SHOR"
      },
      "id": "o0keMH__SHOR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6b4c518-d186-4c4b-92b1-44c5847941e4",
      "metadata": {
        "id": "b6b4c518-d186-4c4b-92b1-44c5847941e4"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "\n",
        "from IPython.display import Audio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4ccaee8-4cea-4f75-8fe7-1425991f7d1a",
      "metadata": {
        "id": "b4ccaee8-4cea-4f75-8fe7-1425991f7d1a"
      },
      "source": [
        "To get started we have a recording of someone playing a few notes on a guitar.\n",
        "\n",
        "The effect we will apply is called Tremolo. You probably have heard it before in a song,\n",
        "but like me might not know it by name.\n",
        "\n",
        "What it does is modulate the amplitude (loudness) of the recording with a low\n",
        "frequency. Typically a few Hertz."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "First, let's load a guitar recording:"
      ],
      "metadata": {
        "id": "HxGJhOtAS5ko"
      },
      "id": "HxGJhOtAS5ko"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49e20ce2-58fc-4949-ab92-e7c964610647",
      "metadata": {
        "id": "49e20ce2-58fc-4949-ab92-e7c964610647"
      },
      "outputs": [],
      "source": [
        "rate, guitar = scipy.io.wavfile.read(\"guitar.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The recording is returned as a sampling `rate` in Herz and the NumPy array `guitar`. We can use the `Audio` widget to listen to it:"
      ],
      "metadata": {
        "id": "Q9Tdcv4YS6t5"
      },
      "id": "Q9Tdcv4YS6t5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e4c1816-3a1d-4332-acda-378234b95565",
      "metadata": {
        "id": "0e4c1816-3a1d-4332-acda-378234b95565"
      },
      "outputs": [],
      "source": [
        "Audio(guitar, rate=rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NumPy Implementation of the Tremolo Effect\n",
        "\n",
        "Below you will find a NumPy implementation of the tremolo effect."
      ],
      "metadata": {
        "id": "A9yN2WT1TSz4"
      },
      "id": "A9yN2WT1TSz4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "092650df-40f9-463b-b079-dc6f5e962e72",
      "metadata": {
        "id": "092650df-40f9-463b-b079-dc6f5e962e72"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def simple_tremolo(audio, frequency, depth, sample_rate=44100):\n",
        "    \"\"\"\n",
        "    Apply a simple tremolo effect to the input audio signal.\n",
        "\n",
        "    Parameters:\n",
        "    - audio (ndarray): Input audio signal\n",
        "    - frequency (float): Frequency of the tremolo effect (Hz)\n",
        "    - depth (float): Magnitude of the tremolo effect (0-1)\n",
        "\n",
        "    Returns:\n",
        "    - ndarray: Output audio signal with tremolo effect\n",
        "    \"\"\"\n",
        "    t = np.arange(len(audio)) / float(sample_rate)\n",
        "    modulator = np.sin(2 * np.pi * frequency * t) # / 2 + 0.5\n",
        "    output = audio * (1 + depth * modulator)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's listen to it applied to our guitar signal.  You can play with the frequency and depths parameters."
      ],
      "metadata": {
        "id": "tqT3sF6oTpKH"
      },
      "id": "tqT3sF6oTpKH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7276a2f5-fc9e-44b1-b8f3-a02656b58919",
      "metadata": {
        "id": "7276a2f5-fc9e-44b1-b8f3-a02656b58919"
      },
      "outputs": [],
      "source": [
        "Audio(simple_tremolo(guitar, 10, 0.65, sample_rate=rate), rate=rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "318b8ca2-52ea-4127-bb0f-b2ccf2f300cb",
      "metadata": {
        "id": "318b8ca2-52ea-4127-bb0f-b2ccf2f300cb"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "A few minutes break for you to implement a version that uses the array API."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below some imports that will be useful.  We also define a function to convert back to NumPy since the `Audio` widget does not work with GPU arrays.\n",
        "\n",
        "**TODO: Need to move the text explaining the helper here, or just not do it, and ask users to insert `.cpu()` or `to_numpy()` manually for the `Audio` widget!?  I do like the idea of introducing it as part of \"visualization\".**\n",
        "\n"
      ],
      "metadata": {
        "id": "2r8IA0XnVrsE"
      },
      "id": "2r8IA0XnVrsE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37db1cd7-aaaf-485d-a166-7971b4d90c0b",
      "metadata": {
        "id": "37db1cd7-aaaf-485d-a166-7971b4d90c0b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import array_api_compat\n",
        "\n",
        "\n",
        "def convert_to_numpy(array, xp=None):\n",
        "    \"\"\"Convert X into a NumPy ndarray on the CPU.\"\"\"\n",
        "    # Note: In the future, `np.from_dlpack()` may be enough for this.\n",
        "    if xp is None:\n",
        "        xp = array_api_compat.get_namespace(array)\n",
        "    xp_name = xp.__name__\n",
        "\n",
        "    if xp_name in {\"array_api_compat.torch\", \"torch\"}:\n",
        "        return array.cpu().numpy()\n",
        "    elif xp_name == \"cupy.array_api\":\n",
        "        return array._array.get()\n",
        "    elif xp_name in {\"array_api_compat.cupy\", \"cupy\"}:\n",
        "        return array.get()\n",
        "\n",
        "    return np.asarray(array)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `guitar` numpy array has been converted to a PyTorch array, but you can also try using CuPy."
      ],
      "metadata": {
        "id": "fycjZN3QVu_4"
      },
      "id": "fycjZN3QVu_4"
    },
    {
      "cell_type": "code",
      "source": [
        "guitar_torch = torch.asarray(guitar, device=\"cuda\")"
      ],
      "metadata": {
        "id": "z7WtH8I8VwM3"
      },
      "id": "z7WtH8I8VwM3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try running the tremolo below with a torch tensor.  Since it is on the GPU, this will fail.  If it was a torch CPU tensor it would succeed but still use NumPy.\n",
        "\n",
        "After trying this, rewrite `simple_tremolo` to be Array API compatible!"
      ],
      "metadata": {
        "id": "CG0baUdTWkoa"
      },
      "id": "CG0baUdTWkoa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3f92cec-e0f5-42b7-b67a-0c3c2c7dcc9a",
      "metadata": {
        "id": "f3f92cec-e0f5-42b7-b67a-0c3c2c7dcc9a"
      },
      "outputs": [],
      "source": [
        "### TODO: This cell should be empty!  (or users just modify the original one)\n",
        "\n",
        "# For this cell, using device = array_api_compat.device(audio) will be a stumbling stone!\n",
        "# That is, if we use torch GPU.  But torch CPU just works :/.\n",
        "#\n",
        "# OR: We set the default torch device to GPU!\n",
        "\n",
        "def simple_tremolo(audio, frequency, depth, sample_rate=44100):\n",
        "    \"\"\"\n",
        "    Apply a simple tremolo effect to the input audio signal.\n",
        "\n",
        "    Parameters:\n",
        "    - audio (ndarray): Input audio signal\n",
        "    - frequency (float): Frequency of the tremolo effect (Hz)\n",
        "    - depth (float): Magnitude of the tremolo effect (0-1)\n",
        "\n",
        "    Returns:\n",
        "    - ndarray: Output audio signal with tremolo effect\n",
        "    \"\"\"\n",
        "    xp = array_api_compat.get_namespace(audio)\n",
        "\n",
        "    device = array_api_compat.device(audio)\n",
        "    t = xp.arange(len(audio), device=device) / float(sample_rate)\n",
        "    modulator = xp.sin(2 * xp.pi * frequency * t) # / 2 + 0.5\n",
        "    output = audio * (1 + depth * modulator)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dba72d24-31f9-45d0-9a20-6266889e9831",
      "metadata": {
        "id": "dba72d24-31f9-45d0-9a20-6266889e9831"
      },
      "outputs": [],
      "source": [
        "tremolo_guitar = simple_tremolo(guitar_torch, 10, 0.65, sample_rate=rate)\n",
        "\n",
        "Audio(convert_to_numpy(tremolo_guitar), rate=rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To show that this worked, try timing both with the %timeit magic! The NumPy version:"
      ],
      "metadata": {
        "id": "1bxICM74XKW_"
      },
      "id": "1bxICM74XKW_"
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit simple_tremolo(guitar, 10, 0.65, sample_rate=rate)"
      ],
      "metadata": {
        "id": "WBYGMCGzXRCt"
      },
      "id": "WBYGMCGzXRCt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And your version with torch or cupy:"
      ],
      "metadata": {
        "id": "nHhv2ugTYEqC"
      },
      "id": "nHhv2ugTYEqC"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Clear this cell again!\n",
        "\n",
        "%timeit simple_tremolo(guitar_torch, 10, 0.65, sample_rate=rate)"
      ],
      "metadata": {
        "id": "QCExiOEHYBlz"
      },
      "id": "QCExiOEHYBlz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e6e3e0e8-db89-4840-8d4b-2cccef30c801",
      "metadata": {
        "id": "e6e3e0e8-db89-4840-8d4b-2cccef30c801"
      },
      "source": [
        "## Visualisation\n",
        "\n",
        "The tremolo effect is nice because you can hear it and it is easy to visualise when looking at the waveform.\n",
        "\n",
        "Let's do some plotting with the `librosa` library that has useful built in visualisation tools.\n",
        "\n",
        "One thing to note is that `librosa` does not use the array API, like `matplotlib`, which means\n",
        "we will need to convert our PyTorch array to a Numpy array. Currently this requires a small\n",
        "library aware conversion function. Different array libraries have different methods for\n",
        "allowing you to convert back to a Numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09527f95-65ce-42a8-8578-d5a45e94ebea",
      "metadata": {
        "id": "09527f95-65ce-42a8-8578-d5a45e94ebea"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "824b9064-bb27-498d-8cf3-6814eee94b71",
      "metadata": {
        "id": "824b9064-bb27-498d-8cf3-6814eee94b71"
      },
      "outputs": [],
      "source": [
        "def waveshow(data, title=\"Amplitude\"):\n",
        "    xp = array_api_compat.get_namespace(data)\n",
        "    data = xp.astype(data, xp.float32)\n",
        "    data /= xp.max(xp.abs(data))\n",
        "\n",
        "    data_np = convert_to_numpy(data, xp)\n",
        "    librosa.display.waveshow(data_np, sr=44100)\n",
        "    plt.title(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "999c3e1b-ae1b-4849-a6d8-5dfb9e6a9306",
      "metadata": {
        "id": "999c3e1b-ae1b-4849-a6d8-5dfb9e6a9306"
      },
      "outputs": [],
      "source": [
        "# Selecting one second from the sample where a note is being played\n",
        "one_second = slice(int(1.5*rate), int(2.5*rate))\n",
        "\n",
        "waveshow(guitar[one_second], title=\"Original guitar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7d2cb43-9550-478f-a12b-0af4de26d69c",
      "metadata": {
        "id": "d7d2cb43-9550-478f-a12b-0af4de26d69c"
      },
      "outputs": [],
      "source": [
        "tremolo_guitar = simple_tremolo(guitar_torch, 10, 0.65, sample_rate=rate)\n",
        "waveshow(tremolo_guitar[one_second], title=\"Tremolo guitar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b13aded-75ad-47bf-8498-6d9939309252",
      "metadata": {
        "id": "6b13aded-75ad-47bf-8498-6d9939309252"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d6f06ee-f1fe-4e10-92b6-fa6d3df137a4",
      "metadata": {
        "id": "6d6f06ee-f1fe-4e10-92b6-fa6d3df137a4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e19c869-db64-47d4-8172-b71de485bb9d",
      "metadata": {
        "id": "2e19c869-db64-47d4-8172-b71de485bb9d"
      },
      "outputs": [],
      "source": [
        "from ipywebrtc import AudioRecorder, CameraStream\n",
        "\n",
        "# On colab, uncomment these lines (or colab will ask later)\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cd685fe-5644-42a9-9ba0-a8e30ff005af",
      "metadata": {
        "id": "5cd685fe-5644-42a9-9ba0-a8e30ff005af"
      },
      "source": [
        "## Recording some audio\n",
        "\n",
        "We need some sound to work on. Luckily we can just record something with the microphone in our computers.\n",
        "\n",
        "Note: if recording a custom audio does not work for you, simply keep using the guitar audio!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6e3366d-508e-48f7-a181-38d8e3373dde",
      "metadata": {
        "id": "a6e3366d-508e-48f7-a181-38d8e3373dde"
      },
      "outputs": [],
      "source": [
        "def record_audio():\n",
        "    camera = CameraStream(constraints={'audio': True, 'video': False})\n",
        "    recorder = AudioRecorder(stream=camera)\n",
        "    return recorder\n",
        "\n",
        "def convert_audio(recorder):\n",
        "    recorder.save(\"recording.webm\")\n",
        "    !ffmpeg -i recording.webm -ac 1 -ar 44100 -f wav my_recording.wav -y -hide_banner -loglevel panic\n",
        "\n",
        "    rate, rec = scipy.io.wavfile.read(\"my_recording.wav\")\n",
        "\n",
        "    return rate, rec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a913bec-b3a5-412a-81d5-0520e21065b2",
      "metadata": {
        "id": "2a913bec-b3a5-412a-81d5-0520e21065b2"
      },
      "outputs": [],
      "source": [
        "recorder = record_audio()\n",
        "recorder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "611eecb1-20a3-44d9-81d8-d664d7c729de",
      "metadata": {
        "id": "611eecb1-20a3-44d9-81d8-d664d7c729de"
      },
      "outputs": [],
      "source": [
        "sample_rate, audio = convert_audio(recorder)\n",
        "\n",
        "# Or use the guitar audio:\n",
        "# sample_rate, audio = scipy.io.wavfile.read(\"guitar.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support for third party widgets will remain active for the duration of the session. To disable support:"
      ],
      "metadata": {
        "id": "pT47EwtfZc1N"
      },
      "id": "pT47EwtfZc1N"
    },
    {
      "cell_type": "markdown",
      "id": "df705c71-f3dd-44dc-ac68-53e07737b139",
      "metadata": {
        "id": "df705c71-f3dd-44dc-ac68-53e07737b139"
      },
      "source": [
        "## Speeding up a recording\n",
        "\n",
        "You all know the \"playback speed\" button on YouTube. Let's implement a simple version of this.\n",
        "\n",
        "When we record sound we create a set of samples. Typically something like 20000 samples per second. This means a one second\n",
        "recording contains about 20000 samples. To play back a recording at the right speed we need to know the sample rate,\n",
        "how many samples were recorded per second.\n",
        "\n",
        "To speed up a recording by ten percent we can take an existing 5second recording made of `100_000` samples and reduce the total number\n",
        "of samples to `100_000 / 1.1 = 90910` samples. When we then play back this smaller number of samples at the same rate, we will get\n",
        "a shorter recording.\n",
        "\n",
        "XXX insert diagram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29c82a0b-24b7-4c21-a457-11269dd42aa5",
      "metadata": {
        "id": "29c82a0b-24b7-4c21-a457-11269dd42aa5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def speed_up_audio(audio_data, factor=1.1):\n",
        "    \"\"\"Speed up recording by interpolation\n",
        "\n",
        "    The total number of samples is reduced by `factor` which leads\n",
        "    to a shorter recording when `factor>1`.\n",
        "    \"\"\"\n",
        "    new_audio = np.interp(\n",
        "        np.arange(0, len(audio_data), factor),\n",
        "        np.arange(len(audio_data)),\n",
        "        audio_data,\n",
        "    )\n",
        "    return new_audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ce7351a-9bc8-4dba-8f93-de902bb6a5fc",
      "metadata": {
        "id": "3ce7351a-9bc8-4dba-8f93-de902bb6a5fc"
      },
      "outputs": [],
      "source": [
        "fast_audio = speed_up_audio(audio, 2.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b6142f2-6f10-4d59-b309-967b579e25dc",
      "metadata": {
        "id": "5b6142f2-6f10-4d59-b309-967b579e25dc"
      },
      "outputs": [],
      "source": [
        "Audio(fast_audio, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ae44a8a-d388-4e31-aa94-a481b09bcdb6",
      "metadata": {
        "id": "0ae44a8a-d388-4e31-aa94-a481b09bcdb6"
      },
      "source": [
        "The basics work, so lets re-implement this using the array API so that it works with CuPy, PyTorch and Numpy arrays.\n",
        "\n",
        "The speed up function looks pretty straightforward so it should be easy to convert it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25daaf8f-ab5e-4b3f-b5c2-f95ff8e40153",
      "metadata": {
        "id": "25daaf8f-ab5e-4b3f-b5c2-f95ff8e40153"
      },
      "outputs": [],
      "source": [
        "import array_api_compat\n",
        "\n",
        "\n",
        "def speed_up_audio(audio_data, factor=1.1):\n",
        "    \"\"\"Speed up recording by interpolation\n",
        "\n",
        "    The total number of samples is reduced by `factor` which leads\n",
        "    to a shorter recording when `factor>1`.\n",
        "    \"\"\"\n",
        "    xp = array_api_compat.get_namespace(audio_data)\n",
        "\n",
        "    new_audio = xp.interp(\n",
        "        xp.arange(0, len(audio_data), factor, device=audio_data.device),\n",
        "        xp.arange(len(audio_data), device=audio_data.device),\n",
        "        audio_data,\n",
        "    )\n",
        "\n",
        "    return new_audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb5bada9-3f3f-4ffb-bd40-0c2547ca73e7",
      "metadata": {
        "id": "fb5bada9-3f3f-4ffb-bd40-0c2547ca73e7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "audio_torch = torch.asarray(audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87046d33-b056-4e34-b4a0-a1b346ca5ff3",
      "metadata": {
        "id": "87046d33-b056-4e34-b4a0-a1b346ca5ff3"
      },
      "outputs": [],
      "source": [
        "speed_up_audio(audio_torch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfdf5b9d-97b5-4276-9189-9c79c600bee5",
      "metadata": {
        "id": "dfdf5b9d-97b5-4276-9189-9c79c600bee5"
      },
      "source": [
        "It is of course not that easy.\n",
        "\n",
        "The array API standard does not cover all functions that exist in Numpy.\n",
        "\n",
        "So we will have to write our own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8520d762-343b-4baf-bf34-dc086fae054f",
      "metadata": {
        "id": "8520d762-343b-4baf-bf34-dc086fae054f"
      },
      "outputs": [],
      "source": [
        "def interp(x, xp, fp):\n",
        "    \"\"\"Interpolate a function at the points `x`\n",
        "\n",
        "    The original function is represented by points `xp` where the function\n",
        "    has the value `fp`. The interpolated result is calculated by interpolating\n",
        "    the points of the function closes to each point in `x`.\n",
        "    \"\"\"\n",
        "    # This ensures all three arrays are from the same namespace\n",
        "    xp_ = array_api_compat.get_namespace(x, xp, fp)\n",
        "\n",
        "    y = xp_.zeros_like(x)\n",
        "    # Assume `x` is sorted, like `xp`\n",
        "    idx = 0\n",
        "    for n, xi in enumerate(x):\n",
        "        if xi < xp[0]:\n",
        "            y[n] = fp[0]\n",
        "        elif xi > xp[-1]:\n",
        "            y[n] = fp[-1]\n",
        "        else:\n",
        "            while xi > xp[idx + 1]:\n",
        "                idx += 1\n",
        "            y[n] = fp[idx] + (fp[idx + 1] - fp[idx]) * (xi - xp[idx]) / (xp[idx + 1] - xp[idx])\n",
        "\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: This is a potential fast(er) interpolation implementation"
      ],
      "metadata": {
        "id": "CWyL0bsWhqBC"
      },
      "id": "CWyL0bsWhqBC"
    },
    {
      "cell_type": "code",
      "source": [
        "def interp_fast(x, xp, fp):\n",
        "    # The Array API does support searchsorted, so we can get a decent speed with it.\n",
        "    # In principle, we don't need a full interpolate since xp is regular.\n",
        "    xp_ = array_api_compat.get_namespace(x, xp, fp)\n",
        "\n",
        "    upper = xp_.searchsorted(xp, x, side=\"right\")\n",
        "    lower = upper - 1\n",
        "    if xp_.any((upper < 1) | (upper >= len(xp))):\n",
        "        raise ValueError(\"Cannot interpolate outside range.\")\n",
        "\n",
        "    spacing = xp[upper] - xp[lower]\n",
        "    spacing[spacing == 0] = 1  # avoid NaN values (should not happen)\n",
        "\n",
        "    frac = (x - xp[lower]) / spacing\n",
        "    return fp[lower] * (1 - frac) + fp[upper] * frac"
      ],
      "metadata": {
        "id": "m4bUZTA7dj5g"
      },
      "id": "m4bUZTA7dj5g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3cdcca70-1697-4ce1-9c5e-c8ad866be1a4",
      "metadata": {
        "id": "3cdcca70-1697-4ce1-9c5e-c8ad866be1a4"
      },
      "source": [
        "Quick little sanity check:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03aa3324-3ebe-45c6-8486-ebf005aaa76c",
      "metadata": {
        "id": "03aa3324-3ebe-45c6-8486-ebf005aaa76c"
      },
      "outputs": [],
      "source": [
        "interp_fast(np.asarray([2, 2.5]), np.asarray([1., 2., 3.]), np.asarray([2., 3, 5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a13581b8-4d01-4fba-a291-194f42ffa957",
      "metadata": {
        "id": "a13581b8-4d01-4fba-a291-194f42ffa957"
      },
      "outputs": [],
      "source": [
        "def speed_up_audio(audio_data, factor=1.1):\n",
        "    \"\"\"Speed up recording by interpolation\n",
        "\n",
        "    The total number of samples is reduced by `factor` which leads\n",
        "    to a shorter recording when `factor>1`.\n",
        "    \"\"\"\n",
        "    xp = array_api_compat.get_namespace(audio_data)\n",
        "\n",
        "    new_audio = interp_fast(\n",
        "        xp.arange(0, len(audio_data), factor, device=audio_data.device),\n",
        "        xp.arange(len(audio_data), device=audio_data.device),\n",
        "        audio_data,\n",
        "    )\n",
        "\n",
        "    return new_audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "142be7e8-f83f-4f50-af92-df8e34be576d",
      "metadata": {
        "id": "142be7e8-f83f-4f50-af92-df8e34be576d"
      },
      "outputs": [],
      "source": [
        "fast_audio_torch = speed_up_audio(audio_torch, 2.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6afb988a-ce33-4eb6-af65-1f0df4411deb",
      "metadata": {
        "id": "6afb988a-ce33-4eb6-af65-1f0df4411deb"
      },
      "outputs": [],
      "source": [
        "# We have to convert the result back to Numpy because the `Audio` widget\n",
        "# does not use the array API :-)\n",
        "Audio(convert_to_numpy(fast_audio_torch), rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9b7bdc9-f483-433a-a77b-8fcfb914c34f",
      "metadata": {
        "id": "c9b7bdc9-f483-433a-a77b-8fcfb914c34f"
      },
      "source": [
        "# Delete below here\n",
        "\n",
        "I can't make the convolution work well enough, without using a lot of memory and funny artefacts in the output audio.\n",
        "So I think we should skip it\n",
        "\n",
        "\n",
        "## Reverb\n",
        "\n",
        "Explain what reverb effect is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30d2a7d0-37e7-4af2-a424-52d1657278ed",
      "metadata": {
        "id": "30d2a7d0-37e7-4af2-a424-52d1657278ed"
      },
      "outputs": [],
      "source": [
        "# Using a clap sound from freesound.org\n",
        "# 909 clap.wav by NoiseCollector -- https://freesound.org/s/3718/ -- License: Attribution 3.0\n",
        "rate_impulse, impulse = scipy.io.wavfile.read(\"3718__noisecollector__909-clap.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c804751c-c5f9-4e3e-b87a-ba9ed396bfe9",
      "metadata": {
        "id": "c804751c-c5f9-4e3e-b87a-ba9ed396bfe9"
      },
      "outputs": [],
      "source": [
        "impulse = impulse[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a62fd615-dbc3-40c8-b093-73db6941c3a3",
      "metadata": {
        "id": "a62fd615-dbc3-40c8-b093-73db6941c3a3"
      },
      "outputs": [],
      "source": [
        "impulse.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16659292-1021-4de6-a41b-c66724f47452",
      "metadata": {
        "id": "16659292-1021-4de6-a41b-c66724f47452"
      },
      "outputs": [],
      "source": [
        "rate_impulse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26638fe0-6ee7-4ed3-89f8-b3a93be7cf1c",
      "metadata": {
        "id": "26638fe0-6ee7-4ed3-89f8-b3a93be7cf1c"
      },
      "outputs": [],
      "source": [
        "def fft_convolution(signal, kernel):\n",
        "    N = len(signal) + len(kernel) - 1\n",
        "\n",
        "    signal = np.pad(signal, (0, N - len(signal)))\n",
        "    kernel = np.pad(kernel, (0, N - len(kernel)))\n",
        "\n",
        "    signal_fft = np.fft.fft(signal)\n",
        "    kernel_fft = np.fft.fft(kernel)\n",
        "\n",
        "    product_fft = signal_fft * kernel_fft\n",
        "\n",
        "    signal_conv = np.real(np.fft.ifft(product_fft))\n",
        "    return signal_conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "494e6e45-d587-4670-83a1-10c207540b02",
      "metadata": {
        "id": "494e6e45-d587-4670-83a1-10c207540b02"
      },
      "outputs": [],
      "source": [
        "#new_audio = fft_convolution(audio, impulse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7208ec91-c13d-4bc0-ae48-d5e7d1dc97b2",
      "metadata": {
        "id": "7208ec91-c13d-4bc0-ae48-d5e7d1dc97b2"
      },
      "outputs": [],
      "source": [
        "from scipy.fftpack import fft, ifft\n",
        "\n",
        "def overlap_add_convolution(signal, filter_kernel, chunk_size=8192):\n",
        "    # Ensure chunk_size is a power of 2 for efficient FFT\n",
        "    chunk_size = 2 ** np.ceil(np.log2(chunk_size)).astype(int)\n",
        "\n",
        "    # Pad filter_kernel to chunk_size\n",
        "    padded_filter_kernel = np.pad(filter_kernel, (0, chunk_size - len(filter_kernel)))\n",
        "\n",
        "    # Initialize output\n",
        "    output = np.zeros_like(signal, dtype=np.float64)\n",
        "\n",
        "    # Process in chunks\n",
        "    for i in range(0, len(signal), chunk_size):\n",
        "        chunk = signal[i:i+chunk_size]\n",
        "        # Pad chunk if necessary\n",
        "        if len(chunk) < chunk_size:\n",
        "            chunk = np.pad(chunk, (0, chunk_size - len(chunk)))\n",
        "\n",
        "        # maybe padd chunk to len(chunk) + len(filter)?\n",
        "        #if len(chunk) < chunk_size + len(padded_filter_kernel):\n",
        "        #    chunk = np.pad(chunk, (0, chunk_size + len(padded_filter_kernel) - len(chunk)))\n",
        "\n",
        "        #N = len(chunk) + len(padded_filter_kernel) - 1\n",
        "        #chunk = np.pad(chunk, (0, N - len(chunk)))\n",
        "        #padded_filter_kernel = np.pad(padded_filter_kernel, (0, N - len(padded_filter_kernel)))\n",
        "\n",
        "        # Perform FFT on chunk and filter_kernel\n",
        "        fft_chunk = fft(chunk)\n",
        "        fft_filter_kernel = fft(padded_filter_kernel)\n",
        "\n",
        "        # Multiply in frequency domain\n",
        "        #print(fft_chunk.shape, fft_filter_kernel.shape)\n",
        "        product = fft_chunk * fft_filter_kernel\n",
        "\n",
        "        # Inverse FFT\n",
        "        result = ifft(product)\n",
        "\n",
        "        # Add to output, handling overlap\n",
        "        start = i\n",
        "        end = start + len(result)\n",
        "        if end > len(output):\n",
        "            end = len(output)\n",
        "        output[start:end] += result[:end-start].real\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "323344f4-2dc4-434e-aaee-9efeb489c091",
      "metadata": {
        "id": "323344f4-2dc4-434e-aaee-9efeb489c091"
      },
      "outputs": [],
      "source": [
        "new_audio = overlap_add_convolution(audio, impulse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff2fe1e1-6fb7-4802-986a-f3b12130824d",
      "metadata": {
        "id": "ff2fe1e1-6fb7-4802-986a-f3b12130824d"
      },
      "outputs": [],
      "source": [
        "Audio(new_audio, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eecd21d-6558-4478-b749-d1d6324506a4",
      "metadata": {
        "id": "2eecd21d-6558-4478-b749-d1d6324506a4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c99111f4-b527-42ba-aa2a-a66989612088",
      "metadata": {
        "id": "c99111f4-b527-42ba-aa2a-a66989612088"
      },
      "outputs": [],
      "source": [
        "new_audio[40_000:40_100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "698331be-fc0c-4fbb-81e5-696d3ca0c165",
      "metadata": {
        "id": "698331be-fc0c-4fbb-81e5-696d3ca0c165"
      },
      "outputs": [],
      "source": [
        "audio[40_000:40_100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0895b83d-901c-45e1-8c4f-a57635221a40",
      "metadata": {
        "id": "0895b83d-901c-45e1-8c4f-a57635221a40"
      },
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "import torchaudio.functional as F\n",
        "\n",
        "# Load the impulse response and normalise\n",
        "rir_raw, sample_rate = torchaudio.load(\"3718__noisecollector__909-clap.wav\")\n",
        "\n",
        "# Convolve speech with room impulse response\n",
        "speech_with_reverb = F.fftconvolve(torch.asarray(audio), rir_raw[0,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ac797a-beee-4e5b-bac0-6bd835ce5213",
      "metadata": {
        "id": "08ac797a-beee-4e5b-bac0-6bd835ce5213"
      },
      "outputs": [],
      "source": [
        "rir_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "454778d7-14de-44e0-88d9-5a451cc63e24",
      "metadata": {
        "id": "454778d7-14de-44e0-88d9-5a451cc63e24"
      },
      "outputs": [],
      "source": [
        "np.max(impulse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db0b03ba-cf84-4b58-b27b-c7fdc4491ee7",
      "metadata": {
        "id": "db0b03ba-cf84-4b58-b27b-c7fdc4491ee7"
      },
      "outputs": [],
      "source": [
        "np.max(audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7163b84-d16b-4891-829d-15405057e910",
      "metadata": {
        "id": "a7163b84-d16b-4891-829d-15405057e910"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4709dcf0-1b9f-40fb-8a26-14c21a20e3c1",
      "metadata": {
        "id": "4709dcf0-1b9f-40fb-8a26-14c21a20e3c1"
      },
      "outputs": [],
      "source": [
        "speech_with_reverb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b7c4b15-313f-4526-bbf0-47b1757a0793",
      "metadata": {
        "id": "3b7c4b15-313f-4526-bbf0-47b1757a0793"
      },
      "outputs": [],
      "source": [
        "Audio(audio, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96139b54-bcef-42d7-9d0c-c0be8387be8e",
      "metadata": {
        "id": "96139b54-bcef-42d7-9d0c-c0be8387be8e"
      },
      "outputs": [],
      "source": [
        "Audio(speech_with_reverb.numpy(), rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ce0d234-841c-4e42-938d-6832267c1e87",
      "metadata": {
        "id": "0ce0d234-841c-4e42-938d-6832267c1e87"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.fftpack import fft, ifft\n",
        "\n",
        "def overlap_add_convolve(signal, filter):\n",
        "    # Parameters\n",
        "    M = len(filter)\n",
        "    N = len(signal)\n",
        "    L = 2 * M  # Block size\n",
        "    print(L)\n",
        "    step_size = L - M + 1  # Step size for overlap-add\n",
        "\n",
        "    # Zero-pad the filter to the block size\n",
        "    filter_padded = np.pad(filter, (0, L - M))\n",
        "\n",
        "    # Initialize the output array\n",
        "    output = np.zeros(N + M - 1)\n",
        "\n",
        "    # Loop through the signal in blocks\n",
        "    for i in range(0, N, step_size):\n",
        "        # Extract the current block of the signal\n",
        "        block = signal[i:i+L]\n",
        "\n",
        "        # Zero-pad the block if necessary\n",
        "        if len(block) < L:\n",
        "            block = np.pad(block, (0, L - len(block)))\n",
        "\n",
        "        # Perform FFT convolution on the block\n",
        "        block_fft = fft(block)\n",
        "        filter_fft = fft(filter_padded)\n",
        "        result_fft = block_fft * filter_fft\n",
        "        result = ifft(result_fft).real\n",
        "        print(result.shape)\n",
        "\n",
        "        # Add the result to the output with appropriate overlap\n",
        "        start_idx = i\n",
        "        end_idx = start_idx + L\n",
        "        if end_idx > output.shape[0]:\n",
        "            delta = end_idx - output.shape[0]\n",
        "            end_idx -= delta\n",
        "\n",
        "            print(start_idx, end_idx, output.shape, (result[:-delta]).shape)\n",
        "            output[start_idx:end_idx] += result[:-delta]\n",
        "        else:\n",
        "            output[start_idx:end_idx] += result\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63d5ff3c-d17a-4ce0-8142-595161285df5",
      "metadata": {
        "id": "63d5ff3c-d17a-4ce0-8142-595161285df5"
      },
      "outputs": [],
      "source": [
        "new_audio = overlap_add_convolve(audio, impulse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4894f604-4709-4508-afac-5db3d569d4ee",
      "metadata": {
        "id": "4894f604-4709-4508-afac-5db3d569d4ee"
      },
      "outputs": [],
      "source": [
        "Audio(new_audio, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0caf50d0-ec4b-4f2f-8cad-6003534b793f",
      "metadata": {
        "id": "0caf50d0-ec4b-4f2f-8cad-6003534b793f"
      },
      "outputs": [],
      "source": [
        "Audio(audio, rate=sample_rate)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}