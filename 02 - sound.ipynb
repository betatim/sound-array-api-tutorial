{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3864244f-6f2a-4a49-aac4-f1aaf74d4012",
      "metadata": {
        "id": "3864244f-6f2a-4a49-aac4-f1aaf74d4012"
      },
      "source": [
        "# Sound\n",
        "\n",
        "How does sound work? How is it recorded? What does it look like?\n",
        "This notebook is about the basics of sound.\n",
        "\n",
        "When we speak we create pressure waves which travel through the air. Your ears can sense these waves. And that is how you can hear what I am saying right now.\n",
        "\n",
        "To record sound and store it in a `.wav` file we measure how \"loud\" (the amplitude) a sound is many, many, many times a second. These measured values get stored in the file, together with information about how often per second they were recorded. You need this to know how long the recording is. The jargon for this is \"sampling rate\". Typically the sampling rate is something like 20000Hz or even 44000Hz.\n",
        "\n",
        "First, we will create a short sound sample that we can use to explore sound."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1ddeb72-5e7a-4d54-b8bd-662fd9705eed",
      "metadata": {
        "id": "f1ddeb72-5e7a-4d54-b8bd-662fd9705eed"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio\n",
        "\n",
        "\n",
        "sample_rate = 22050  # Hz\n",
        "frequency1 = 512  # Hz\n",
        "frequency2 = 1024  # Hz\n",
        "play_time = 1  # seconds\n",
        "\n",
        "# Generate time array\n",
        "t = np.arange(0, play_time, 1/sample_rate)\n",
        "\n",
        "# Generate sound waves\n",
        "wave1 = np.sin(2 * np.pi * frequency1 * t) * np.exp(-t/0.7)\n",
        "wave2 = np.sin(2 * np.pi * frequency2 * t) * np.exp(-t/0.7)\n",
        "\n",
        "# Create silence array\n",
        "silence = np.zeros(int(sample_rate * play_time))\n",
        "\n",
        "# Concatenate sound waves and silence\n",
        "sound = np.concatenate((silence, wave1, silence, wave2))\n",
        "\n",
        "noise = np.random.normal(0, 0.01, len(sound))\n",
        "sound_with_noise = sound + noise\n",
        "sound_with_noise /= np.max(np.abs(sound_with_noise))\n",
        "\n",
        "# Save to file\n",
        "import scipy.io.wavfile as wavfile\n",
        "wavfile.write(\"output.wav\", sample_rate, sound)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47b7e73f-f615-40bf-988e-5f3308a18761",
      "metadata": {
        "id": "47b7e73f-f615-40bf-988e-5f3308a18761"
      },
      "outputs": [],
      "source": [
        "Audio(sound, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb2a940a-fce0-4479-be47-f0a5b1a05139",
      "metadata": {
        "id": "fb2a940a-fce0-4479-be47-f0a5b1a05139"
      },
      "outputs": [],
      "source": [
        "Audio(sound_with_noise, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ljRPx_OUZX9w",
      "metadata": {
        "id": "ljRPx_OUZX9w"
      },
      "source": [
        "Let's look at the amplitude of this sound recording. It should be kind of quiet, then show a sound, then be quiet and then another sound."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90787654-d471-4cc1-97c8-ba9d263e493b",
      "metadata": {
        "id": "90787654-d471-4cc1-97c8-ba9d263e493b"
      },
      "outputs": [],
      "source": [
        "import librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb389d43-32af-4bcd-93a9-0d6b60e32e0a",
      "metadata": {
        "id": "eb389d43-32af-4bcd-93a9-0d6b60e32e0a"
      },
      "outputs": [],
      "source": [
        "librosa.display.waveshow(sound_with_noise)\n",
        "plt.title(\"Amplitude of Sound Sample\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pvj4YyxLZmwB",
      "metadata": {
        "id": "pvj4YyxLZmwB"
      },
      "source": [
        "We can see what we expected.\n",
        "\n",
        "But what we can't see from this is that the two sounds are different to our ear. The reason they are different is because they are at different frequencies. But we can't see that by looking at the amplitude as a function of time.\n",
        "\n",
        "There is a tool for this. The Fourier transform. It allows you to\n",
        "look at the sound's amplitude as a function of frequency, instead of time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "905cac88-e2fa-4941-9217-431481869681",
      "metadata": {
        "id": "905cac88-e2fa-4941-9217-431481869681"
      },
      "outputs": [],
      "source": [
        "sp = np.fft.fft(sound_with_noise)\n",
        "\n",
        "freq = np.fft.fftfreq(sound_with_noise.shape[0], d=1/sample_rate)\n",
        "idx = np.where((freq > 0) & (freq <= 2000))[0]\n",
        "\n",
        "plt.plot(freq[idx], np.abs(sp[idx]))\n",
        "plt.xlabel('Frequency (Hz)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.title('Fourier Transform of Sound Sample');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iwnITRt6bVx2",
      "metadata": {
        "id": "iwnITRt6bVx2"
      },
      "source": [
        "We see that the sound contains two main frequencies. One at around 500Hz and one around 1000Hz.\n",
        "\n",
        "This matches what we'd expect, afterall we created this sound by adding two sounds, one at 512Hz and one at 1024Hz.\n",
        "\n",
        "We can also create a spectrogram. This shows you both amplitude as a function of time and frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "476388bb-59fe-42f9-9d40-fa290e3382a5",
      "metadata": {
        "id": "476388bb-59fe-42f9-9d40-fa290e3382a5"
      },
      "outputs": [],
      "source": [
        "D = librosa.stft(sound_with_noise)\n",
        "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "\n",
        "librosa.display.specshow(S_db, x_axis='time', y_axis='log')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2c2e2eb-7456-4358-88f0-f06e948b5a56",
      "metadata": {
        "id": "b2c2e2eb-7456-4358-88f0-f06e948b5a56"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Performing Fourier transforms is possible with the array API and generally a good use of a GPU.\n",
        "\n",
        "Fourier transform, show that GPU FFT is quicker, pull forward"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install array-api-compat"
      ],
      "metadata": {
        "id": "LgZAeDFOxjRq"
      },
      "id": "LgZAeDFOxjRq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f590fdae-409d-4241-bb37-a894ab5f91f9",
      "metadata": {
        "id": "f590fdae-409d-4241-bb37-a894ab5f91f9"
      },
      "outputs": [],
      "source": [
        "import array_api_compat\n",
        "\n",
        "\n",
        "def convert_to_numpy(array, xp=None):\n",
        "    \"\"\"Convert X into a NumPy ndarray on the CPU.\"\"\"\n",
        "    # Note: In the future, `np.from_dlpack()` may be enough for this.\n",
        "    if xp is None:\n",
        "        xp = array_api_compat.get_namespace(array)\n",
        "    xp_name = xp.__name__\n",
        "\n",
        "    if xp_name in {\"array_api_compat.torch\", \"torch\"}:\n",
        "        return array.cpu().numpy()\n",
        "    elif xp_name == \"cupy.array_api\":\n",
        "        return array._array.get()\n",
        "    elif xp_name in {\"array_api_compat.cupy\", \"cupy\"}:\n",
        "        return array.get()\n",
        "\n",
        "    return np.asarray(array)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rewrite this function so it works with Numpy, CuPy, PyTorch\n",
        "\n",
        "def plot_fft(audio):\n",
        "    sp = np.fft.fft(audio)\n",
        "\n",
        "    freq = np.fft.fftfreq(audio.shape[0], d=1/sample_rate)\n",
        "    idx = np.where((freq > 0) & (freq <= 2000))[0]\n",
        "\n",
        "    plt.plot(convert_to_numpy(freq[idx]), convert_to_numpy(np.abs(sp[idx])))\n",
        "    plt.xlabel('Frequency (Hz)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.title('Fourier Transform of Sound Sample');"
      ],
      "metadata": {
        "id": "pEdAHtM3xtm_"
      },
      "id": "pEdAHtM3xtm_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4152aba5-8455-4f0d-b606-14574ea2f8f6",
      "metadata": {
        "id": "4152aba5-8455-4f0d-b606-14574ea2f8f6"
      },
      "outputs": [],
      "source": [
        "plot_fft(sound_with_noise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c287bcc7-02ed-4890-85af-c310e666eab1",
      "metadata": {
        "id": "c287bcc7-02ed-4890-85af-c310e666eab1"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4da82efd-80eb-4258-908b-5f09a0336e6d",
      "metadata": {
        "id": "4da82efd-80eb-4258-908b-5f09a0336e6d"
      },
      "outputs": [],
      "source": [
        "sound_with_noise_ = torch.asarray(sound_with_noise, device=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8585efcb-7142-4af2-930d-60328ac53ac6",
      "metadata": {
        "id": "8585efcb-7142-4af2-930d-60328ac53ac6"
      },
      "outputs": [],
      "source": [
        "plot_fft(sound_with_noise_)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}